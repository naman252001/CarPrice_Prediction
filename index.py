# -*- coding: utf-8 -*-
"""Cars_Price_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V2Dcuoo3THxojWG0AlCcLeSPzU_1WI_F

# **Car Price Prediction**

### **Problem Statement**
A Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.

They have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:

- Which variables are significant in predicting the price of a car
- How well those variables describe the price of a car

Based on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.

### **Business Goal**
You are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

cars = pd.read_csv('/content/CarPrice_Assignment.csv')
cars.head()

pd.set_option('display.max_columns',None)
cars.head()

cars.shape

cars.describe()

cars.info()

cars['CarName'].unique()

# splitting company name from carname column
CompanyName = cars['CarName'].apply(lambda x:x.split(' ')[0])
cars.insert(3,'CompanyName',CompanyName)
cars.drop(['CarName'],axis = 1,inplace =True)
cars.head()

cars['CompanyName'].unique()

# maxda, Nissan, porcshce, toyouta, vokswagen, vw

cars.CompanyName = cars.CompanyName.str.lower()

def replace_name(a,b):
  cars.CompanyName.replace(a,b,inplace=True)

replace_name('maxda','mazda')
replace_name('porcshce','porsche')
replace_name('toyouta','toyota')
replace_name('vokswagen','volkswagen')
replace_name('vw','volkswagen')

cars.CompanyName.unique()

cars.columns

"""Visualizing the Data"""

plt.figure(figsize=(20,8))
plt.subplot(1,2,1)
plt.title('Car Price Distribution Plot')
sns.distplot(cars.price)
plt.subplot(1,2,2)
plt.title('Car Price Spread')
sns.boxplot(y=cars.price)
plt.show()

print(cars.price.describe(percentiles = [.25,.50,.75,.85,.9,1]))

"""Inference:
1. The plot seemed to be right skewed, meaning that the most of the prices in the dataset are low.
2. There is a significant difference between the mean and median of the price distribution.
3. The data points are far spread out from the mean, which indicates a high variance in the car prices.( 85% of the prices are below 18500 whereas remaining 15% are between 18500 and 45400.)

Visualizing the categorical Data


*   ComapanyName
*   Symboling
*   feultype
*   enginetype
*   carbody
*   doornumber
*   enginelocation
*   feulsystem
*   aspiration
*   drivewheel
"""

## Make a bar graph of Comapany name, fuel type, car type

plt.subplot(1,3,1)
plt1 = cars.CompanyName.value_counts().plot(kind='bar')
plt.title('Companies Histogram')
plt1.set(xlabel = 'Car Company',ylabel = 'Frequency of company')

plt.subplot(1,3,2)
plt1 = cars.fueltype.value_counts().plot(kind= 'bar')
plt.title('Fuel Types Histogram')
plt1.set(xlabel = 'Fuel type',ylabel = 'Frequency of fuel type')

plt.subplot(1,3,3)
plt1 = cars.carbody.value_counts().plot(kind= 'bar')
plt.title('car type Histogram')
plt1.set(xlabel = 'car type',ylabel = 'Frequency of car type')

"""Inference:
1. Toyota seemed to be favoured car company
2. Number of gas fueled cars are more than diesel
3. Sedan is the top prefered car type.
"""

# Making a bar graph of Symboling and also compare symboling w.r.t price

cars.head()

plt.figure(figsize =(20,8))
plt.subplot(1,2,1)
plt.title('Symboling Histogram')
cars['symboling'].value_counts().plot(kind = 'bar')

plt.subplot(1,2,2)
plt.title('Symboling vs Price')
sns.boxplot(x = cars.symboling,y=cars.price)

"""Inference:
1. It seems that the symboling with 0 and 1 have high number of car sold
2. The car with -1 symboling seems to be high priced (as it make sense too, risk rating -1 is quite good). But it seems that symboling with 3 values has the price range similar to -2 which is unfair. There is a dip in price at symboling 1.
"""

##  Analyze Engine type w.r.t price

plt.figure(figsize =(20,8))
plt.subplot(1,2,1)
plt.title('Engine Type')
cars['enginetype'].value_counts().plot(kind = 'bar')

plt.subplot(1,2,2)
plt.title('engine type vs Price')
sns.boxplot(x = cars.enginetype,y=cars.price)
plt.show()

df = pd.DataFrame(cars.groupby(['enginetype'])['price'].mean().sort_values(ascending = False))
df.plot.bar(figsize=(8,6))
plt.title('engine type vs avg Price')

"""Inference:
1. ohc Engine type seemed to be most favourable type and average price of ohc engine type is least amongst other.
2. ohcv has the higest price range while dohcv have only 1 row and with most hih average price.
"""

## compare avg price wrt company name,fuel type and carbody

plt.figure(figsize =(25,6))
df = pd.DataFrame(cars.groupby(['CompanyName'])['price'].mean().sort_values(ascending = False))
df.plot.bar()
plt.title('Company Name vs Average Price')
plt.show()

df = pd.DataFrame(cars.groupby(['fueltype'])['price'].mean().sort_values(ascending = False))
df.plot.bar()
plt.title('Fuel Type vs Avg Price')
plt.show()

df = pd.DataFrame(cars.groupby(['carbody'])['price'].mean().sort_values(ascending = False))
df.plot.bar()
plt.title('Car Body vs Avg Price')
plt.show()

"""Inference:
1. Jaguar and Buick seems to have high avg price.
2. Diesel has higher avg price than gas
3. Hardtop and convertible have higher avg price.
"""

## Analyse Door number and aspiration

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
plt.title('Door Number Histogram')
sns.countplot(x=cars.doornumber,palette = ('plasma'))

plt.subplot(1,2,2)
plt.title('Door number vs price')
sns.boxplot(x = cars.doornumber, y = cars.price, palette=('plasma'))
plt.show()

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
plt.title('Aspiration Histogram')
sns.countplot(x=cars.aspiration,palette = ('plasma'))

plt.subplot(1,2,2)
plt.title('aspiration vs price')
sns.boxplot(x = cars.aspiration, y = cars.price, palette=('plasma'))
plt.show()

"""Inference:
1.  Cars with 4Doornumber is sold more.
2. Doornumbwer variable is not affecting  the price much.
3. It seems that aspiration with turbo have higher price range than std(tough it has some high values outside the whishkers).
"""

def plot_count(x,fig):
  plt.subplot(4,2,fig)
  plt.title(x+'Histogram')
  sns.countplot(x=cars[x],palette=('magma'))
  plt.subplot(4,2,fig+1)
  plt.title(x+'vs Price')
  sns.boxplot(x=cars[x],y = cars.price,palette = ('magma'))


plt.figure(figsize=(15,20))

plot_count('enginelocation',1)
plot_count('cylindernumber',3)
plot_count('fuelsystem',5)
plot_count('drivewheel',7)
plt.tight_layout()

"""Inference:
1. cars with four enginelocation is sold more than rear but rear enginelocation have more costly than front.
2. four cylindernumber is also more favourable than other and six and eight cylinder number is more costly.
3. fwd drivewheel is more favourable and rwd is costly than other.

# Visualising Numerical Data
"""

def scatter (x,fig):
  plt.subplot(5,2,fig)
  plt.scatter(cars[x],cars['price'])
  plt.title(x+'vs Price')
  plt.ylabel('Price')
  plt.xlabel(x)

plt.figure(figsize=(10,20))
scatter('carlength',1)
scatter('carwidth',2)
scatter('carheight',3)
scatter('curbweight',4)
plt.tight_layout()

"""Inference:
1. carwidth,carlength and curbweight seems to have a positive correlation with price.
2. carheight doesn't show any significant trend with price.
"""

def pp(x,y,z):
  sns.pairplot(cars,x_vars=[x,y,z],y_vars='price',size=4,kind='scatter')
  plt.show()

pp('enginesize','boreratio','stroke')
pp('compressionratio','horsepower','peakrpm')
pp('wheelbase','citympg','highwaympg')

"""Inference:
1. enginesize, boreratio,horsepower,wheelbase seems to have positive correlaion with price.
2. citympg,highwaympg-seem to have negative correlation with price.
"""

# Fuel economy

cars['fueleconomy'] = (.55*cars['citympg'] + (0.45*cars['highwaympg']))

cars.head()

# Binning the cars companies based on avg prices of each company
cars['price'] = cars['price'].astype('int')
temp = cars.copy()
table=temp.groupby(['CompanyName'])['price'].mean()
temp = temp.merge(table.reset_index(),how ='left',on='CompanyName')
bins=[0,10000,20000,40000]
cars_bin= ['Budget','Medium','Highend']
cars['carsrange']= pd.cut(temp['price_y'],bins,right=False,labels=cars_bin)
cars.head()

"""List of significant variables after Various Visual Analysis:

- Engine Type
- Car Rnage
- Fuel Type
- Car Body
- Aspiration
- Cylinder Number
- Drivewheel
- Curbweight
- car length
-car width
- Engine size
- Bore ratio
- Horse power
- Wheel bhase
- Fuel Economy
"""

cars_lr = cars[['price','fueltype','aspiration','carbody','drivewheel','wheelbase','curbweight','enginetype','cylindernumber','enginesize',
                'boreratio','horsepower','fueleconomy','carlength','carwidth','carsrange']]
cars_lr.head()

print(cars['drivewheel'].unique())
print(cars['aspiration'].unique())
print(cars['carbody'].unique())
print(cars['drivewheel'].unique())
print(cars['enginetype'].unique())
print(cars['cylindernumber'].unique())
print(cars['carsrange'].unique())

## Encoding

def dummies(x,df):
  temp = pd.get_dummies(df[x],drop_first=True)
  df = pd.concat([df,temp],axis=1)
  df.drop([x],axis=1,inplace =True)
  return df

cars_lr = dummies('fueltype',cars_lr)
cars_lr = dummies('aspiration',cars_lr)
cars_lr = dummies('carbody',cars_lr)
cars_lr = dummies('drivewheel',cars_lr)
cars_lr = dummies('enginetype',cars_lr)
cars_lr = dummies('cylindernumber',cars_lr)
cars_lr = dummies('carsrange',cars_lr)

cars_lr.head()

"""# Train-Test Split and feature scaling"""

# split the data in x and y
x = cars_lr.drop('price',axis=1)
y = cars_lr['price']

# train test split

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,train_size = 0.8)

print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)

# Scaling the data
from sklearn.preprocessing import StandardScaler

x.head()

for i in['wheelbase','curbweight','enginesize','boreratio','horsepower','fueleconomy','carlength','carwidth']:

  sc = StandardScaler()
  x_train[i] = sc.fit_transform(pd.DataFrame(x_train[i]))
  x_test[i] = sc.fit_transform(pd.DataFrame(x_test[i]))

x_train.head()

"""Model Building
1. We will train the model on x_train and y_train and then the model on x_test
2. Compare the performance using y_test

OVERFITTING AND UNDERFITTING conditios:
1. If training error and testing error has large variance then it is case of overfitting.
2. In overfitting condition, machine learns very well on training data but it perform bad on testing data.
3. If the train werror and test error are high then this is the case of underfitting.
4. In underfitting condition, machine learning does not learn well on training data as well as doesnot perform well on testing data
"""

# Create a base model and check for a assumption

import statsmodels.api as sm

x_train_c = sm.add_constant(x_train)
x_test_c = sm.add_constant(x_test)

base_model = sm.OLS(y_train,x_train_c).fit()

print(base_model.summary())

"""### **First part model Interpretation**

1. Dep. Variable - Here dependent variable is price that we are going to predict through model
2. Method - Least squares.
3. Model - OLS stand for ordinary least.
4. No. of observation. - Total no of observation present in training datset.
5. Df Residuals - sample size minus the number of parameters being estimated, n-k-1.
6. Df Model = k = number of variables (without constant).
7. R-squared - R2 is a statistic that will gives infirmation about the goodness of fit of a model. It range from 0 to 1. In our Case the values of r2 is .95 so it means 95% of variance is explained by the model.
8. Adj. R-squared - Each time yoy add an independent variable to the model, the r2 increase, even if the independent variable is not significant, it never decrease.
Whereas Adj R-squared increases only when the independent variable is significant and affect the dependent variable.
9. F-statistic and prob(F-statistic) - It check the overall significance of model.
- F-stats ----> F-table is necessary
- Prob(F-stats) ----->
- H0 : The model is insignificant.
- H1 : The model is significant
10. AIC & BIC - Akaike's Information criteria is usde for model selection. It penalises the errors made in case a new variable is added to regression equation. A lower AIC implies a better model. BIC stand for Bayesia information criteria and is variant of AIC whee penalities are made more server.

## **Second Part Interpretation**

1. Constant term : The constant term is the intercept of regression line.
2. Coefficient term : The coeff. term tells the change in Y for a unit chnge in X. i.e. if X rises by 1 unit then Y reises by 600 unit for wheelbase variable.
Price = Constant+*wheelbase.
3. Std. Error : It shows the accuracy for each prediction. Lower the std error better the estimates.
4. t : coeff/std.error
5. p(t) : pvalue< 0.05 is consideres as significant variable.
6. confidence interval:
- H0: The variable is significant(p val in between interval)
- H1: The variable is significant(p val not in between interval)

## ** Third Part Interpretation**

1. Omnibus/Prob(Omnibus) - To check whether errors are normally distributed or not.
2. Skew - It is a measurement of summetry in data.
3. Kurtosis - Measures peakedness of data.
4. Durbin Watson - Homoscedasticity Test.
5. Jarque Bera/prob(JB) - Test for normality of residual.
6. Condition Number - Multicollinearity.
. if multicollinearity <100 = no multicoll
. multicoll == 100 - 1000 = low multicoll
. multicoll > 1000 = high multicoll
"""

if base_model.f_pvalue < 0.5:
  print('Reject H0: The model is significant')

else:
  print('Fail to reject H0: The model is insignificant')

base_model.pvalues

# significant features in the model
for i in base_model.pvalues.to_dict():
  if base_model.pvalues.to_dict()[i]<0.05:
    print(i)

# to see the relation of features with target variable
base_model.params[1:].sort_values().plot(kind='bar')

"""Assumption of Linear Regression"""

resid = base_model.resid

sns.distplot(resid)

from scipy.stats import shapiro

_,p = shapiro(resid)
if p>0.05:
  print('HO: The data is Normal')
else:
  print('H1: The data is not Normal')

"""Multicollinrarity"""

# condition no.
# heat map.
# VIF (vaiance inflation factor)           >>>    (if VIF > 10= HIGH multicoll)       >>>     (VIF(==5 - 10) = low Multicoll)      >>>     (VIF < 5 = no multicoll)

"""Homoscedasticity"""

# breush pagan test
# H0 : There is homoscedasticity
# H1: There is Heteroscedasticity

from statsmodels.stats.diagnostic  import het_breuschpagan

_,p,_,_ = het_breuschpagan(resid,base_model.model.exog)

p

# p > 0.05, There is homoscedasticiy

"""Autocorrelation"""

# Relation within errors
# Durbin watson test   >>>> (less than 1.5= positive) >>>> (between 1.5 to 2.5 = no autocorrelation) >>>> (more than 2.5 = negative)

"""# Sklearn Method"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error

lr = LinearRegression()
model_lr = lr.fit(x_train,y_train)
pred_train = model_lr.predict(x_train)
pred_test = model_lr.predict(x_test)

r2_train = r2_score(y_train,pred_train)
r2_test = r2_score(y_test,pred_test)
print('R2 Train: ',r2_train)
print('R2 Test: ',r2_test)

# Mean squared
print('MSE Train: ',mean_squared_error(y_train,pred_train))
print('MSE Test: ',mean_squared_error(y_test,pred_test))
print('RMSE Train: ',np.sqrt(mean_squared_error(y_train,pred_train)))
print('RMSE Test: ',np.sqrt(mean_squared_error(y_test,pred_test)))

plt.figure(figsize=(10,6))
plt.scatter(y_test,pred_test)
plt.plot([min(y_test),max(y_test)],[min(y_test),max(y_test)],linestyle='--',color='r',linewidth=2)
plt.title('Actual Price Vs Predicted Price')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Prices')
plt.show

"""Gradient Descent

"""

from sklearn.linear_model import SGDRegressor

model_sgd = SGDRegressor()

model_sgd.fit(x_train,y_train)

y_pred_sgd = model_sgd.predict(x_test)

from sklearn.metrics import r2_score,mean_squared_error

print('R square: ',r2_score(y_test,y_pred_sgd))
print('MSE: ',mean_squared_error(y_test,y_pred_sgd))
print('RMSE: ',np.sqrt(mean_squared_error(y_test,y_pred_sgd)))

"""Feature Scaling"""

from mlxtend.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LinearRegression

sfs = SequentialFeatureSelector(estimator = LinearRegression(),k_features='best',cv = 3, scoring='r2')

sfs.fit(x_train,y_train)

sel_feature = list(sfs.k_feature_names_)
sel_feature

model_sfs = LinearRegression()

model_sfs.fit(x_train[sel_feature],y_train)

y_pred_sfs = model_sfs.predict(x_test[sel_feature])

print('R square: ',r2_score(y_test,y_pred_sfs))
print('MSE: ',mean_squared_error(y_test,y_pred_sfs))
print('RMSE: ',np.sqrt(mean_squared_error(y_test,y_pred_sfs)))

"""Regularisation Techniques - Ridge, Lasso, Elastic Net"""

from sklearn.linear_model import Ridge,Lasso,ElasticNet

model_ridge = Ridge(alpha=100)

model_ridge.fit(x_train,y_train)

y_pred_ridge = model_ridge.predict(x_test)

print('R square: ',r2_score(y_test,y_pred_ridge))
print('MSE: ',mean_squared_error(y_test,y_pred_ridge))
print('RMSE: ',np.sqrt(mean_squared_error(y_test,y_pred_ridge)))

from sklearn.model_selection import GridSearchCV

grid = {'alpha':[.001,.01,.1,1,10,100,1000]}
gscv = GridSearchCV(estimator = Ridge(),param_grid=grid,cv=3,scoring='r2',verbose=3)

gscv.fit(x_train,y_train)

gscv.best_params_

model_ridge = Ridge(**gscv.best_params_)

model_ridge.fit(x_train,y_train)

y_pred_ridge = model_ridge.predict(x_test)

print('R square: ',r2_score(y_test,y_pred_ridge))
print('MSE: ',mean_squared_error(y_test,y_pred_ridge))
print('RMSE: ',np.sqrt(mean_squared_error(y_test,y_pred_ridge)))

